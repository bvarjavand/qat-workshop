name: llm_03_quantize
debug: false
environment:
  environment_variables:
    - NCCL_DEBUG=INFO
  image: 
    "bvarjavandhpe/workshops:ngc_0.2"
resources:
  resource_pool: A100
  slots_per_trial: 1
searcher:
  name: single
  max_length:
    batches: 5000
  metric: eval_accuracy
  smaller_is_better: false
hyperparameters:
  model: "mistralai/Mistral-7B-v0.1"
  dataset: "HuggingFaceH4/ultrachat_200k"
  dryrun: true
  training_args:
    output_dir: "/tmp/MistralQAT"
    overwrite_output_dir: true
    bf16: true, # specify fp16: true instead when training on GPUs that don't support bf16
    do_eval: true,
    evaluation_strategy: "epoch"
    gradient_accumulation_steps: 128
    gradient_checkpointing: true
    gradient_checkpointing_kwargs: 
        "use_reentrant": false
    learning_rate: 2.0e-05
    log_level: "info"
    logging_steps: 5
    logging_strategy: "steps"
    lr_scheduler_type: "cosine"
    max_steps: 100
    num_train_epochs: 1
    per_device_eval_batch_size: 1 # originally set to 8
    per_device_train_batch_size: 1 # originally set to 8
    seed: 42
    save_strategy: "steps"
    save_steps: 50
entrypoint: >-
  python -m determined.launch.torch_distributed
  python finetune.py
max_restarts: 0